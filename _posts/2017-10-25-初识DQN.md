---
layout: post
title: "初始DQN"
tags:
---
前两天AlphaGo Zero的新闻震惊全世界，加上那个吸引人的标题”Mastering the Game of Go without Human Knowledege“，引得我这个门外汉也去把论文打印出来读了一遍。完全云里雾里。就记得有个Q(s,a),比蒙特卡罗搜索树MCST印象还要深。所以又去找了DeepMind的另一篇论文“Playing Atari with Deep Reinforcement Learning”来了解一下。
还是没有完全看懂，把知道的一知半解记录在下面，方便后面复习：
* DQN是Q-learning的一个特例，使用**Deep Convolutional Network**来学习Q函数
* Q-learning算法的关键是理解**Bellman equation**![alt Bellman-equation](https://wikimedia.org/api/rest_v1/media/math/render/svg/0fe1ed37fa6eb19e26b55529c42f9d65b8f14a35)
* DQN使用**Experience Replay**方法，记录固定长度的历史数据，集中到一个pool中，然后随机从这个pool选择样本用于训练深度卷积网络(其实才5层，A
lphaGo Zero里的深度残差大约有100层)。

后面需要理解Q-learning算法，帮助理解这个**Deep Q-Networks**
